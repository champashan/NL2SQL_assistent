# -*- coding: utf-8 -*-
"""NL2SQL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mPAJlTLtAtSRmYZRTi7eseJGhMIIQHnz

NL2SQL Assistant
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
# %cd /content/drive/My Drive/

!git clone --recurse-submodules https://github.com/HKUNLP/UnifiedSKG.git

# Commented out IPython magic to ensure Python compatibility.
# %cd UnifiedSKG

import sys
sys.path.append('/content/drive/My Drive/UnifiedSKG')

!pip install googletrans==3.1.0a0

!pip install transformers==4.9.2
!pip install nltk
!pip install datasets==1.14.0
!pip install sentencepiece
!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html

import os
import time
import torch
import datasets
from googletrans import Translator 
from transformers import (
    HfArgumentParser,
    set_seed,
    AutoTokenizer
)
from utils.configue import Configure
from utils.training_arguments import WrappedSeq2SeqTrainingArguments

from filelock import FileLock
import nltk
with FileLock(".lock") as lock:
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)

sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', 
            '--cfg', 'Salesforce/T5_base_prefix_spider_with_cell_value.cfg', 
            '--output_dir', './tmp']
parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))
training_args, = parser.parse_args_into_dataclasses()
set_seed(training_args.seed)
args = Configure.Get(training_args.cfg)

tokenizer = AutoTokenizer.from_pretrained("hkunlp/from_all_T5_base_prefix_spider_with_cell_value2", use_fast=False)
from models.unified.prefixtuning import Model
model = Model(args)
model.load("hkunlp/from_all_T5_base_prefix_spider_with_cell_value2")

translator = Translator()
inpt = input("Введите запрос на естественном языке:\n")
question = translator.translate(inpt, dest='en')
print(question.text)

#struct_in = "| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id"
text_in = question.text

translator = Translator()
print("=====Запрос=====")
print(inpt)
tokenized_txt = tokenizer([text_in], max_length=1024, padding="max_length", truncation=True)
pred = tokenizer.batch_decode(
    model.generate(
      torch.LongTensor(tokenized_txt.data['input_ids']),
      torch.LongTensor(tokenized_txt.data['attention_mask']),
      num_beams=1, 
      max_length=256
      ), 
    skip_special_tokens=True 
) 
print("=====Ответ=====")
print(*pred)

!pip3 install pytelegrambotapi

import telebot

bot = telebot.TeleBot('5420372192:AAExGe9r0icqAXTGYUSjBpKJjzUMPZZFiWg')

@bot.message_handler(commands=["start"])

def start(m, res=False):

    bot.send_message(m.chat.id, 'NL2SQL Assistant запущен!')
    bot.send_message(m.chat.id, 'Напишите ваш запрос на естественном языке:')

def nlsql(inpt):
  translator = Translator()
  question = translator.translate(inpt, dest='en')
  text_in = question.text
  translator = Translator()

  tokenized_txt = tokenizer([text_in], max_length=1024, padding="max_length", truncation=True)
  pred = tokenizer.batch_decode(
      model.generate(
        torch.LongTensor(tokenized_txt.data['input_ids']),
        torch.LongTensor(tokenized_txt.data['attention_mask']),
        num_beams=1, 
        max_length=256
        ), 
      skip_special_tokens=True 
  ) 
  
  return str(*pred)

# Получение сообщений от пользователя

@bot.message_handler(content_types=["text"])

def handle_text(message):

    bot.send_message(message.chat.id, 'Ваш запрос:\n' + message.text + '\n' + '\nПеревод запроса в SQL query:\n' + nlsql(message.text))


bot.polling(none_stop=True, interval=0)